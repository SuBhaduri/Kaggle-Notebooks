{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Fraud identification problem is one of the cases where we will have imbalanced dataset i.e the fraud transactions will be very few in numbers. So if we don't apply specific techniques we will not be able to get a proper model.\n\n#### In this notebook we will start applying standard approach of a classification problem and see why it doesn't work and what will be the alternative approaches.\n\n#### Import the libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import DataFrame\n%matplotlib inline","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Explore the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.info()","execution_count":44,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### So 'Class' is the target variable and let's see how it is distributed in the dataset and then we can understand why it is called a imbalanced dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Class'].value_counts()","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"0    284315\n1       492\nName: Class, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Class', data=data)","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7effee835450>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"#### We can visualize the number of entries with Class=1 (Fraud) is very less compared to Class=0 (Not-Fraud).\n\n#### Next we will split the data into train_set and validation_set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data, val_data = train_test_split(data, test_size=0.25, random_state=42)\nprint(train_data.shape, val_data.shape)","execution_count":47,"outputs":[{"output_type":"stream","text":"(213605, 31) (71202, 31)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Since all V* columns are already scaled we will scale only the 'Time' and 'Amount' columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = train_data['Class']\nval_label   = val_data['Class']\ntrain_data  = train_data.drop(['Class'], axis=1)\nval_data    = val_data.drop(['Class'], axis=1)\nprint(train_data.shape, val_data.shape, train_label.shape, val_label.shape)","execution_count":48,"outputs":[{"output_type":"stream","text":"(213605, 30) (71202, 30) (213605,) (71202,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(2)","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"          Time        V1        V2        V3        V4        V5        V6  \\\n83225  59741.0 -1.648591  1.228130  1.370169 -1.735542 -0.029455 -0.484129   \n52800  45648.0 -0.234775 -0.493269  1.236728 -2.338793 -1.176733  0.885733   \n\n             V7        V8        V9  ...       V20       V21       V22  \\\n83225  0.918645 -0.438750  0.982144  ...  0.384201 -0.218076 -0.203458   \n52800 -1.960981 -2.363412 -2.694774  ...  0.364679 -1.495358 -0.083066   \n\n            V23       V24       V25       V26       V27       V28  Amount  \n83225 -0.213015  0.011372 -0.304481  0.632063 -0.262968 -0.099863   38.42  \n52800  0.074612 -0.347329  0.541900 -0.433294  0.089293  0.212029   61.20  \n\n[2 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83225</th>\n      <td>59741.0</td>\n      <td>-1.648591</td>\n      <td>1.228130</td>\n      <td>1.370169</td>\n      <td>-1.735542</td>\n      <td>-0.029455</td>\n      <td>-0.484129</td>\n      <td>0.918645</td>\n      <td>-0.438750</td>\n      <td>0.982144</td>\n      <td>...</td>\n      <td>0.384201</td>\n      <td>-0.218076</td>\n      <td>-0.203458</td>\n      <td>-0.213015</td>\n      <td>0.011372</td>\n      <td>-0.304481</td>\n      <td>0.632063</td>\n      <td>-0.262968</td>\n      <td>-0.099863</td>\n      <td>38.42</td>\n    </tr>\n    <tr>\n      <th>52800</th>\n      <td>45648.0</td>\n      <td>-0.234775</td>\n      <td>-0.493269</td>\n      <td>1.236728</td>\n      <td>-2.338793</td>\n      <td>-1.176733</td>\n      <td>0.885733</td>\n      <td>-1.960981</td>\n      <td>-2.363412</td>\n      <td>-2.694774</td>\n      <td>...</td>\n      <td>0.364679</td>\n      <td>-1.495358</td>\n      <td>-0.083066</td>\n      <td>0.074612</td>\n      <td>-0.347329</td>\n      <td>0.541900</td>\n      <td>-0.433294</td>\n      <td>0.089293</td>\n      <td>0.212029</td>\n      <td>61.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data.head(2)","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"          Time         V1        V2         V3        V4         V5        V6  \\\n43428  41505.0 -16.526507  8.584972 -18.649853  9.505594 -13.793819 -2.832404   \n49906  44261.0   0.339812 -2.743745  -0.134070 -1.385729  -1.451413  1.015887   \n\n              V7        V8        V9  ...       V20       V21       V22  \\\n43428 -16.701694  7.517344 -8.507059  ... -1.514923  1.190739 -1.127670   \n49906  -0.524379  0.224060  0.899746  ...  0.506044 -0.213436 -0.942525   \n\n            V23       V24       V25       V26       V27       V28  Amount  \n43428 -2.358579  0.673461 -1.413700 -0.462762 -2.018575 -1.042804  364.19  \n49906 -0.526819 -1.156992  0.311211 -0.746647  0.040996  0.102038  520.12  \n\n[2 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>43428</th>\n      <td>41505.0</td>\n      <td>-16.526507</td>\n      <td>8.584972</td>\n      <td>-18.649853</td>\n      <td>9.505594</td>\n      <td>-13.793819</td>\n      <td>-2.832404</td>\n      <td>-16.701694</td>\n      <td>7.517344</td>\n      <td>-8.507059</td>\n      <td>...</td>\n      <td>-1.514923</td>\n      <td>1.190739</td>\n      <td>-1.127670</td>\n      <td>-2.358579</td>\n      <td>0.673461</td>\n      <td>-1.413700</td>\n      <td>-0.462762</td>\n      <td>-2.018575</td>\n      <td>-1.042804</td>\n      <td>364.19</td>\n    </tr>\n    <tr>\n      <th>49906</th>\n      <td>44261.0</td>\n      <td>0.339812</td>\n      <td>-2.743745</td>\n      <td>-0.134070</td>\n      <td>-1.385729</td>\n      <td>-1.451413</td>\n      <td>1.015887</td>\n      <td>-0.524379</td>\n      <td>0.224060</td>\n      <td>0.899746</td>\n      <td>...</td>\n      <td>0.506044</td>\n      <td>-0.213436</td>\n      <td>-0.942525</td>\n      <td>-0.526819</td>\n      <td>-1.156992</td>\n      <td>0.311211</td>\n      <td>-0.746647</td>\n      <td>0.040996</td>\n      <td>0.102038</td>\n      <td>520.12</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler_Time   = StandardScaler()\nstd_scaler_Amount = StandardScaler()\n\ntrain_data['Time']   = std_scaler_Time.fit_transform(train_data[['Time']])\ntrain_data['Amount'] = std_scaler_Amount.fit_transform(train_data[['Amount']])\n\nval_data['Time']   = std_scaler_Time.transform(val_data[['Time']])\nval_data['Amount'] = std_scaler_Amount.transform(val_data[['Amount']])","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(2)","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"           Time        V1        V2        V3        V4        V5        V6  \\\n83225 -0.738270 -1.648591  1.228130  1.370169 -1.735542 -0.029455 -0.484129   \n52800 -1.035079 -0.234775 -0.493269  1.236728 -2.338793 -1.176733  0.885733   \n\n             V7        V8        V9  ...       V20       V21       V22  \\\n83225  0.918645 -0.438750  0.982144  ...  0.384201 -0.218076 -0.203458   \n52800 -1.960981 -2.363412 -2.694774  ...  0.364679 -1.495358 -0.083066   \n\n            V23       V24       V25       V26       V27       V28    Amount  \n83225 -0.213015  0.011372 -0.304481  0.632063 -0.262968 -0.099863 -0.196016  \n52800  0.074612 -0.347329  0.541900 -0.433294  0.089293  0.212029 -0.107223  \n\n[2 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83225</th>\n      <td>-0.738270</td>\n      <td>-1.648591</td>\n      <td>1.228130</td>\n      <td>1.370169</td>\n      <td>-1.735542</td>\n      <td>-0.029455</td>\n      <td>-0.484129</td>\n      <td>0.918645</td>\n      <td>-0.438750</td>\n      <td>0.982144</td>\n      <td>...</td>\n      <td>0.384201</td>\n      <td>-0.218076</td>\n      <td>-0.203458</td>\n      <td>-0.213015</td>\n      <td>0.011372</td>\n      <td>-0.304481</td>\n      <td>0.632063</td>\n      <td>-0.262968</td>\n      <td>-0.099863</td>\n      <td>-0.196016</td>\n    </tr>\n    <tr>\n      <th>52800</th>\n      <td>-1.035079</td>\n      <td>-0.234775</td>\n      <td>-0.493269</td>\n      <td>1.236728</td>\n      <td>-2.338793</td>\n      <td>-1.176733</td>\n      <td>0.885733</td>\n      <td>-1.960981</td>\n      <td>-2.363412</td>\n      <td>-2.694774</td>\n      <td>...</td>\n      <td>0.364679</td>\n      <td>-1.495358</td>\n      <td>-0.083066</td>\n      <td>0.074612</td>\n      <td>-0.347329</td>\n      <td>0.541900</td>\n      <td>-0.433294</td>\n      <td>0.089293</td>\n      <td>0.212029</td>\n      <td>-0.107223</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data.head(2)","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"           Time         V1        V2         V3        V4         V5  \\\n43428 -1.122333 -16.526507  8.584972 -18.649853  9.505594 -13.793819   \n49906 -1.064290   0.339812 -2.743745  -0.134070 -1.385729  -1.451413   \n\n             V6         V7        V8        V9  ...       V20       V21  \\\n43428 -2.832404 -16.701694  7.517344 -8.507059  ... -1.514923  1.190739   \n49906  1.015887  -0.524379  0.224060  0.899746  ...  0.506044 -0.213436   \n\n            V22       V23       V24       V25       V26       V27       V28  \\\n43428 -1.127670 -2.358579  0.673461 -1.413700 -0.462762 -2.018575 -1.042804   \n49906 -0.942525 -0.526819 -1.156992  0.311211 -0.746647  0.040996  0.102038   \n\n         Amount  \n43428  1.073794  \n49906  1.681590  \n\n[2 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>43428</th>\n      <td>-1.122333</td>\n      <td>-16.526507</td>\n      <td>8.584972</td>\n      <td>-18.649853</td>\n      <td>9.505594</td>\n      <td>-13.793819</td>\n      <td>-2.832404</td>\n      <td>-16.701694</td>\n      <td>7.517344</td>\n      <td>-8.507059</td>\n      <td>...</td>\n      <td>-1.514923</td>\n      <td>1.190739</td>\n      <td>-1.127670</td>\n      <td>-2.358579</td>\n      <td>0.673461</td>\n      <td>-1.413700</td>\n      <td>-0.462762</td>\n      <td>-2.018575</td>\n      <td>-1.042804</td>\n      <td>1.073794</td>\n    </tr>\n    <tr>\n      <th>49906</th>\n      <td>-1.064290</td>\n      <td>0.339812</td>\n      <td>-2.743745</td>\n      <td>-0.134070</td>\n      <td>-1.385729</td>\n      <td>-1.451413</td>\n      <td>1.015887</td>\n      <td>-0.524379</td>\n      <td>0.224060</td>\n      <td>0.899746</td>\n      <td>...</td>\n      <td>0.506044</td>\n      <td>-0.213436</td>\n      <td>-0.942525</td>\n      <td>-0.526819</td>\n      <td>-1.156992</td>\n      <td>0.311211</td>\n      <td>-0.746647</td>\n      <td>0.040996</td>\n      <td>0.102038</td>\n      <td>1.681590</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Create a standard function to display model performance metrices."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, classification_report\n\ndef model_def(model, model_name, m_train_data, m_train_label):\n    model.fit(m_train_data, m_train_label)\n    s = \"predict_\"\n    p = s + model_name\n    p = model.predict(m_train_data)\n    cm = confusion_matrix(m_train_label, p)\n    print(\"Confusion Matrix: \\n\", cm)\n    cr = classification_report(m_train_label, p, target_names=['Not Fraud', 'Fraud'])\n    print(\"Classification Report: \\n\", cr)\n    precision = np.diag(cm)/np.sum(cm, axis=0)\n    recall    = np.diag(cm)/np.sum(cm, axis=1)\n    F1 = 2 * np.mean(precision) * np.mean(recall)/(np.mean(precision) + np.mean(recall))\n    cv_score = cross_val_score(model, m_train_data, m_train_label, cv=10, scoring='recall')\n    print(\"Mean CV Score     :\", cv_score.mean())\n    print(\"Std Dev CV Score  :\", cv_score.std())","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's apply Logistic Regreesion algorithm on the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', C=0.5)\nmodel_def(logreg, \"logreg\", train_data, train_label)","execution_count":55,"outputs":[{"output_type":"stream","text":"Confusion Matrix: \n [[213199     27]\n [   137    242]]\nClassification Report: \n               precision    recall  f1-score   support\n\n   Not Fraud       1.00      1.00      1.00    213226\n       Fraud       0.90      0.64      0.75       379\n\n    accuracy                           1.00    213605\n   macro avg       0.95      0.82      0.87    213605\nweighted avg       1.00      1.00      1.00    213605\n\nMean CV Score     : 0.6305120910384068\nStd Dev CV Score  : 0.07487696695249359\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Here you can see the problem. Our overall model accuracy is 1.00 but Recall for Fraud class is just 0.64 which means there is lots of misclassification for Fraud class. As per the confusion matrix out (242 + 137) = 379 Fraud class only 242 are classified properly and this has happened because of imbalanced representation of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_logreg = logreg.predict(val_data)\nprint(\"Logistic Regression: \\n\", confusion_matrix(val_label, val_data_logreg))","execution_count":65,"outputs":[{"output_type":"stream","text":"Logistic Regression: \n [[71077    12]\n [   45    68]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that just 68 out of 113 are predicted properly."},{"metadata":{},"cell_type":"markdown","source":"#### Here is the alternative approach - creating synthetic data i.e the data that does not exist in the original dataset and there are 2 main techinques to do this - NearMiss and SMOTE."},{"metadata":{},"cell_type":"markdown","source":"### **NearMiss Algorithm â€“ Undersampling**\n\n#### NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, it removes the instances of the majority class to increase the spaces between the two classes. \n\n#### The basic intuition about the working of near-neighbor methods is as follows:\n\n#### Step 1: The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, majority class is to be under-sampled.\n#### Step 2: Then, n instances of the majority class that have the smallest distances to those in the minority class are selected.\n#### Step 3: If there are k instances in the minority class, the nearest method will result in k * n instances of the majority class.\n\nReference : https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import NearMiss\n\nprint(\"Before Undersampling, counts of label '1': {}\".format(sum(train_label == 1))) \nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(train_label == 0))) \n  \nnr = NearMiss() \n  \ntrain_data_miss, train_label_miss = nr.fit_sample(train_data, train_label.ravel()) \n  \nprint(\"After Undersampling, counts of label '1': {}\".format(sum(train_label_miss == 1))) \nprint(\"After Undersampling, counts of label '0': {}\".format(sum(train_label_miss == 0))) ","execution_count":57,"outputs":[{"output_type":"stream","text":"Before Undersampling, counts of label '1': 379\nBefore Undersampling, counts of label '0': 213226 \n\nAfter Undersampling, counts of label '1': 379\nAfter Undersampling, counts of label '0': 379\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### So we can see count of majority class i.e Class=0 is reduced to the same count of Class=1 and the dataset has become balanced.\n \n#### Now we will apply Logistic Regression with the same parameters on this undersampled data."},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_miss = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', C=0.5)\nmodel_def(logreg_miss, \"logreg_miss\", train_data_miss, train_label_miss)","execution_count":58,"outputs":[{"output_type":"stream","text":"Confusion Matrix: \n [[377   2]\n [ 24 355]]\nClassification Report: \n               precision    recall  f1-score   support\n\n   Not Fraud       0.94      0.99      0.97       379\n       Fraud       0.99      0.94      0.96       379\n\n    accuracy                           0.97       758\n   macro avg       0.97      0.97      0.97       758\nweighted avg       0.97      0.97      0.97       758\n\nMean CV Score     : 0.915647226173542\nStd Dev CV Score  : 0.04200061001317491\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_miss   = logreg_miss.predict(val_data)\nprint(\"Logistic Regression - Undersampling: \\n\", confusion_matrix(val_label, val_data_miss))","execution_count":59,"outputs":[{"output_type":"stream","text":"Logistic Regression - Undersampling: \n [[47844 23245]\n [    9   104]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Here we see the improvement - 104 out of 113 are predicted properly. So the model has become more accurate but at the cost of low precision."},{"metadata":{},"cell_type":"markdown","source":"### **SMOTE (Synthetic Minority Oversampling Technique) â€“ Oversampling**\n\n#### SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem.\n#### It aims to balance class distribution by randomly increasing minority class examples by replicating them.\n#### SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class.\n\nReference - https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before Oversampling, counts of label '1': {}\".format(sum(train_label == 1))) \nprint(\"Before Oversampling, counts of label '0': {} \\n\".format(sum(train_label == 0))) \n  \nsm = SMOTE(random_state=42) \n  \ntrain_data_SMOTE, train_label_SMOTE = sm.fit_sample(train_data, train_label.ravel()) \n  \nprint(\"After Oversampling, counts of label '1': {}\".format(sum(train_label_SMOTE == 1))) \nprint(\"After Oversampling, counts of label '0': {}\".format(sum(train_label_SMOTE == 0))) ","execution_count":60,"outputs":[{"output_type":"stream","text":"Before Oversampling, counts of label '1': 379\nBefore Oversampling, counts of label '0': 213226 \n\nAfter Oversampling, counts of label '1': 213226\nAfter Oversampling, counts of label '0': 213226\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### So we can see count of minority class i.e Class=1 is increased to the same count of Class=0 and the dataset has become balanced.\n \n#### Now we will apply Logistic Regression with the same parameters on this oversampled data."},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_SMOTE = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto', C=0.5)\nmodel_def(logreg_SMOTE, \"logreg_SMOTE\", train_data_SMOTE, train_label_SMOTE)","execution_count":61,"outputs":[{"output_type":"stream","text":"Confusion Matrix: \n [[207724   5502]\n [ 16522 196704]]\nClassification Report: \n               precision    recall  f1-score   support\n\n   Not Fraud       0.93      0.97      0.95    213226\n       Fraud       0.97      0.92      0.95    213226\n\n    accuracy                           0.95    426452\n   macro avg       0.95      0.95      0.95    426452\nweighted avg       0.95      0.95      0.95    426452\n\nMean CV Score     : 0.9225610371179387\nStd Dev CV Score  : 0.002101615392933071\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_SMOTE  = logreg_SMOTE.predict(val_data)\nprint(\"Logistic Regression - Oversampling: \\n\", confusion_matrix(val_label, val_data_SMOTE))","execution_count":62,"outputs":[{"output_type":"stream","text":"Logistic Regression - Oversampling: \n [[69217  1872]\n [    8   105]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Here we see the real advantage - 105 out of 113 are predicted properly. So the model has become more accurate and look at the Precision compared to that of NearMiss algorithm. So by far this is the most effective model."},{"metadata":{},"cell_type":"markdown","source":"#### Since the most effective model by far is SMOTE and where the datset has become almost doubled we will fit this dataset to an Artificial Neural Network (ANN) to see how it goes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Neural Network\ndef build_classifier():\n    classifier = Sequential([Dense(128, activation='relu', input_shape=(train_data_SMOTE.shape[1], )),\n                             Dropout(rate=0.1),\n                             Dense(64, activation='relu'),\n                             Dropout(rate=0.1),\n                             Dense(1, activation='sigmoid')])\n\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Precision', 'Recall'])\n    print(classifier.summary())\n    return classifier\n\nmodel = KerasClassifier(build_fn=build_classifier)","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_data_SMOTE, train_label_SMOTE,\n                    batch_size=30,\n                    epochs=10,\n                    validation_data=(val_data, val_label))","execution_count":74,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 128)               3968      \n_________________________________________________________________\ndropout (Dropout)            (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 12,289\nTrainable params: 12,289\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 426452 samples, validate on 71202 samples\nEpoch 1/10\n426452/426452 [==============================] - 34s 81us/sample - loss: 0.0180 - Precision: 0.9938 - Recall: 0.9953 - val_loss: 0.0126 - val_Precision: 0.3713 - val_Recall: 0.8938\nEpoch 2/10\n426452/426452 [==============================] - 33s 77us/sample - loss: 0.0057 - Precision: 0.9980 - Recall: 0.9993 - val_loss: 0.0131 - val_Precision: 0.4115 - val_Recall: 0.8850\nEpoch 3/10\n426452/426452 [==============================] - 33s 78us/sample - loss: 0.0041 - Precision: 0.9986 - Recall: 0.9995 - val_loss: 0.0140 - val_Precision: 0.4900 - val_Recall: 0.8673\nEpoch 4/10\n426452/426452 [==============================] - 33s 78us/sample - loss: 0.0035 - Precision: 0.9989 - Recall: 0.9996 - val_loss: 0.0136 - val_Precision: 0.6087 - val_Recall: 0.8673\nEpoch 5/10\n426452/426452 [==============================] - 34s 80us/sample - loss: 0.0031 - Precision: 0.9990 - Recall: 0.9996 - val_loss: 0.0144 - val_Precision: 0.5179 - val_Recall: 0.8938\nEpoch 6/10\n426452/426452 [==============================] - 34s 79us/sample - loss: 0.0026 - Precision: 0.9992 - Recall: 0.9997 - val_loss: 0.0142 - val_Precision: 0.5600 - val_Recall: 0.8673\nEpoch 7/10\n426452/426452 [==============================] - 33s 77us/sample - loss: 0.0026 - Precision: 0.9992 - Recall: 0.9997 - val_loss: 0.0154 - val_Precision: 0.6443 - val_Recall: 0.8496\nEpoch 8/10\n426452/426452 [==============================] - 33s 78us/sample - loss: 0.0025 - Precision: 0.9993 - Recall: 0.9998 - val_loss: 0.0175 - val_Precision: 0.6329 - val_Recall: 0.8850\nEpoch 9/10\n426452/426452 [==============================] - 33s 77us/sample - loss: 0.0023 - Precision: 0.9993 - Recall: 0.9997 - val_loss: 0.0172 - val_Precision: 0.5475 - val_Recall: 0.8673\nEpoch 10/10\n426452/426452 [==============================] - 33s 78us/sample - loss: 0.0020 - Precision: 0.9994 - Recall: 0.9998 - val_loss: 0.0178 - val_Precision: 0.6554 - val_Recall: 0.8584\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_Neural = model.predict(val_data)\nprint(\"Artificial Neural Network: \\n\", confusion_matrix(val_label, val_data_Neural))","execution_count":75,"outputs":[{"output_type":"stream","text":"Artificial Neural Network: \n [[71038    51]\n [   16    97]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### So we can see in the ANN model the Precision is highest but the Recall is compromised slightly as compared to SMOTE."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}